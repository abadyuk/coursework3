{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import metrics \n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, GlobalMaxPooling1D, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "#Заведомо определим гиперпарметры текста в моделях \n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.3\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "# Use English stemmer.\n",
    "word_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функции для лексиграфической очистки всех строк в данных\n",
    "def decontracted(string):\n",
    "    # specific\n",
    "    string = str(string)\n",
    "    string = re.sub(r\"won\\'t\", \"will not\", string)\n",
    "    string = re.sub(r\"can\\'t\", \"can not\", string)\n",
    "\n",
    "    # general\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'s\", \" is\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\"\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    return string\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Cleaning of dataset\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    string = string.str.lower()\n",
    "    \n",
    "    string = string.apply(lambda elem: decontracted(elem))\n",
    "    \n",
    "    #remove special characters\n",
    "    string = string.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
    "    \n",
    "    # remove numbers\n",
    "    string = string.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    #remove stopwords\n",
    "    string = string.apply(lambda x: ' '.join([word.strip() for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "def clean_string(string):\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    string = string.lower()\n",
    "    string = re.sub(r'([^\\w\\s]|_)','', string)\n",
    "    \n",
    "    text = [word.strip() for word in string.split() if word not in stop]\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Функция для препроцессинга данных из датасета в готовые для обучения выборки \n",
    "def textpreproc(data, columns): \n",
    "    texts = []\n",
    "    labels = []\n",
    "     \n",
    "    texts = clean_str(data[columns[0]]) + \" \" + clean_str(data[columns[1]])\n",
    "    labels = data[columns[2]]\n",
    "                  \n",
    "                      \n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Padding input sequences\n",
    "    texts = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of texts tensor:', texts.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(texts,labels,\n",
    "                                                 random_state = 42, test_size=VALIDATION_SPLIT, shuffle=True)\n",
    "                           \n",
    "    return X_train, X_test, y_train, y_test, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка и формирование вектора эмбеддингов\n",
    "def get_embeddings(path, embeddings_index):\n",
    "  wv_from_bin = KeyedVectors.load_word2vec_format(path, binary=True, limit=500000) \n",
    "  #extracting word vectors from google news vector\n",
    "  for word, vector in zip(wv_from_bin.index_to_key, wv_from_bin.vectors):\n",
    "      coefs = np.asarray(vector, dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "  \n",
    "  return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_index = get_embeddings('GoogleNews-vectors-negative300.bin', embeddings_index)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция по созданию матрицы эмбеддингов для входного слоя сетей \n",
    "def embedd_matrix(embeddings_index, tokenizer): \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая модель на основе сверточной нейронной сети\n",
    "def cnn_net(embedding_matrix):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Non-trainable embeddidng layer\n",
    "    model.add(Embedding(embedding_matrix.shape[0], output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 250 , activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель на основе сети LSTM\n",
    "def lstm_net(embedding_matrix):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Non-trainable embeddidng layer\n",
    "    model.add(Embedding(embedding_matrix.shape[0], output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    \n",
    "    model.add(LSTM(units=128 , return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=64))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units = 32 , activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm(embedding_matrix):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Conv1D(32, 4, activation='relu', padding='same'))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.1))          \n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(16, 8, activation=\"relu\", padding='same'))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(16, 8, activation=\"relu\", padding='same'))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Произведем тест наших моделей на датасете Fake News ...\n",
    "news_true = pd.read_csv(\"True_ISOT.csv\")\n",
    "news_fake = pd.read_csv(\"Fake_ISOT.csv\")\n",
    "\n",
    "news_true.insert(4, 'is_true', 1)\n",
    "news_fake.insert(4, 'is_true', 0)\n",
    "\n",
    "news1 = pd.concat([news_true, news_fake])\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1, tokenizer1 = textpreproc(news1, ['title', 'text', 'is_true'])\n",
    "embed_matrix1 = embedd_matrix(embeddings_index, tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " covid = pd.read_excel('COVID_news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid.text.apply(lambda x: len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "news_true = pd.read_csv(\"True_ISOT.csv\")\n",
    "news_fake = pd.read_csv(\"Fake_ISOT.csv\")\n",
    "news1 = pd.concat([news_true, news_fake])\n",
    "\n",
    "\n",
    "\n",
    "ax, fig = plt.subplots(figsize=[11,6])\n",
    "sns.countplot(x=\"subject\", data=news1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,2,figsize=(19,5))\n",
    "g1 = sns.countplot(news1.label,ax=ax[0]);\n",
    "g1.set_title(\"Count of real and fake data\")\n",
    "g1.set_ylabel(\"Count\")\n",
    "g1.set_xlabel(\"Target\")\n",
    "g2 = plt.pie(data[\"target\"].value_counts().values,explode=[0,0],labels=data.target.value_counts().index, autopct='%1.1f%%',colors=['SkyBlue','PeachPuff'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_cnn1 = cnn_net(embed_matrix1)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "filepath=\"weights_cnn1.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_cnn1 = model_cnn1.fit(X_train1, y_train1, batch_size = batch_size ,\n",
    "                              validation_data = (X_test1,y_test1) , epochs = epochs, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm = cnn_net(embed_matrix1)\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8,10))\n",
    "\n",
    "axs[0].plot(history_cnn1.history['accuracy'])\n",
    "axs[0].plot(history_cnn1.history['val_accuracy'])\n",
    "axs[0].set_title('CNN-based model accuracy on 1st Dataset')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'test'], loc='lower right')\n",
    "# summarize history for loss\n",
    "axs[1].plot(history_cnn1.history['loss'])\n",
    "axs[1].plot(history_cnn1.history['val_loss'])\n",
    "axs[1].set_title('CNN-based model loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model_lstm1 = lstm_net(embed_matrix1)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "filepath=\"weights_lstm1.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_lstm1 = model_lstm1.fit(X_train1, y_train1, batch_size = batch_size , \n",
    "                              validation_data = (X_test1,y_test1) , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8,10))\n",
    "\n",
    "axs[0].plot(history_lstm1.history['accuracy'])\n",
    "axs[0].plot(history_lstm1.history['val_accuracy'])\n",
    "axs[0].set_title('LSTM-based model accuracy on 1st Dataset')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'test'], loc='lower right')\n",
    "# summarize history for loss\n",
    "axs[1].plot(history_lstm1.history['loss'])\n",
    "axs[1].plot(history_lstm1.history['val_loss'])\n",
    "axs[1].set_title('LSTM-based model loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "###LOAD BEST then calc accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "covid = pd.read_excel('COVID_news.xlsx')\n",
    "\n",
    "X_train,X_test,y_train,y_test, tokenizer1 = textpreproc(covid, ['title', 'text', 'label'])\n",
    "\n",
    "embed_matrix1 = embedd_matrix(embeddings_index, tokenizer1)\n",
    "cnn_net_covid = cnn_net(embed_matrix1)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "filepath=\"weights_cnn2.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_cnn1 = cnn_net_covid.fit(X_train, y_train, batch_size = batch_size ,\n",
    "                              validation_data = (X_test,y_test) , epochs = epochs, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "covid = pd.read_excel('COVID_news.xlsx')\n",
    "\n",
    "X_train,X_test,y_train,y_test, tokenizer1 = textpreproc(covid, ['title', 'text', 'label'])\n",
    "\n",
    "embed_matrix1 = embedd_matrix(embeddings_index, tokenizer1)\n",
    "lstm_net_covid = lstm_net(embed_matrix1)\n",
    "\n",
    "batch_size = 15\n",
    "epochs = 10\n",
    "\n",
    "filepath=\"weights_lstm2.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_cnn1 = lstm_net_covid.fit(X_train, y_train, batch_size = batch_size ,\n",
    "                              validation_data = (X_test,y_test) , epochs = epochs, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
